{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e338e74",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-24T19:49:30.519192Z",
     "iopub.status.busy": "2025-10-24T19:49:30.518765Z",
     "iopub.status.idle": "2025-10-24T19:49:32.568171Z",
     "shell.execute_reply": "2025-10-24T19:49:32.567057Z"
    },
    "papermill": {
     "duration": 2.055269,
     "end_time": "2025-10-24T19:49:32.569951",
     "exception": false,
     "start_time": "2025-10-24T19:49:30.514682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/sample_submission.csv\n",
      "/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\n",
      "/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279a15a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:49:32.576301Z",
     "iopub.status.busy": "2025-10-24T19:49:32.575803Z",
     "iopub.status.idle": "2025-10-24T19:49:39.928611Z",
     "shell.execute_reply": "2025-10-24T19:49:39.926737Z"
    },
    "papermill": {
     "duration": 7.357911,
     "end_time": "2025-10-24T19:49:39.930606",
     "exception": false,
     "start_time": "2025-10-24T19:49:32.572695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters: {'pca__n_components': 30, 'ridge__alpha': 10.0}\n",
      "Estimated CV(Cross-Validation) RMSE (on transformed target): 2.1214\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "#rmse function computes rmse between y_true and y_pred\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv('/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv')\n",
    "\n",
    "#Clip all negative costs to zero\n",
    "y_train_capped = df_train['Transport_Cost'].clip(lower=0)\n",
    "y_train = np.log1p(y_train_capped) \n",
    "# Apply log(1+y) transformation to improve normality and reduce skewed data\n",
    "\n",
    "df_train = df_train.drop('Transport_Cost', axis=1)#Drop target variable Transport_Cost from train dataframe\n",
    "hospital_ids = df_test['Hospital_Id']\n",
    "\n",
    "df_combined = pd.concat([df_train.drop(['Hospital_Id', 'Supplier_Name'], axis=1),\n",
    "                         df_test.drop(['Hospital_Id', 'Supplier_Name'], axis=1)],\n",
    "                        ignore_index=True)\n",
    "\n",
    "\n",
    "date_cols = ['Order_Placed_Date', 'Delivery_Date']\n",
    "for col in date_cols:\n",
    "    df_combined[col] = pd.to_datetime(df_combined[col], format='%m/%d/%y', errors='coerce')\n",
    "    \n",
    "#Adding new features to dataset and dropping unnecesary features\n",
    "df_combined['Delivery_Time_Days'] = (df_combined['Delivery_Date'] - df_combined['Order_Placed_Date']).dt.days\n",
    "df_combined.loc[df_combined['Delivery_Time_Days'] < 0, 'Delivery_Time_Days'] = np.nan\n",
    "\n",
    "\n",
    "df_combined['Order_Year'] = df_combined['Order_Placed_Date'].dt.year\n",
    "df_combined['Order_Month'] = df_combined['Order_Placed_Date'].dt.month\n",
    "df_combined['Order_DayOfWeek'] = df_combined['Order_Placed_Date'].dt.dayofweek\n",
    "df_combined = df_combined.drop(date_cols, axis=1)\n",
    "\n",
    "\n",
    "df_combined = df_combined.drop('Hospital_Location', axis=1)\n",
    "\n",
    "\n",
    "num_cols = ['Equipment_Height', 'Equipment_Width', 'Equipment_Weight', 'Equipment_Value']\n",
    "for col in num_cols:\n",
    "    df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')\n",
    "\n",
    "df_combined['Equipment_Volume'] = df_combined['Equipment_Height'] * df_combined['Equipment_Width']\n",
    "df_combined['Density'] = df_combined['Equipment_Weight'] / (df_combined['Equipment_Volume']+ 1e-6)\n",
    "df_combined['Value_per_Weight'] = df_combined['Equipment_Value'] / (df_combined['Equipment_Weight'] + 1e-6)\n",
    "#Density=mass/Volume 1e-6 added to handle zero Equipment_Volume present in dataset \n",
    "#Similarly 1e-6 added to handle zero Equipment_Weight in the dataset\n",
    "\n",
    "# Separate back into train and test sets after adding new features ,removing unnecesary features , clipping costs to zero and applying transformations\n",
    "X_train = df_combined.iloc[:len(y_train)]\n",
    "X_test = df_combined.iloc[len(y_train):]\n",
    "\n",
    "\n",
    "#Finding numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Define  pipeline  for numerical features : Median Imputation\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])#Scaling important for PCA+Ridge\n",
    "\n",
    "# Define pipeline for categorical features: Impute 'missing', then One-Hot Encoding is done\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "#  Create PCA + Ridge Regression Pipeline \n",
    "\n",
    "pca_ridge_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(random_state=42)),\n",
    "    ('ridge', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "#  Hyperparameter grid used for tuning hyperparameters\n",
    "param_grid = {'pca__n_components': [5, 10, 20, 30],'ridge__alpha': [0.1, 1.0, 10.0, 100.0]} # PCA components: search for the optimal number of components\n",
    "# Ridge regularization strength (alpha): search for the best control parameter\n",
    "neg_mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "#  Grid Search and Optimization \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pca_ridge_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=neg_mse_scorer,\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train) #Fit the PCA+Ridge model with training data and check which hyperparameters are best\n",
    "\n",
    "# Output best parameters and estimated RMSE\n",
    "best_rmse = np.sqrt(-grid_search.best_score_)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Estimated CV(Cross-Validation) RMSE (on transformed target): {best_rmse:.4f}\")\n",
    "\n",
    "#  Predict transformed transport cost\n",
    "y_pred_transformed = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Reverse the log transformation and ensure non-negative costs\n",
    "y_pred_final = np.expm1(y_pred_transformed).clip(min=0)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Hospital_Id': hospital_ids,\n",
    "    'Transport_Cost': y_pred_final\n",
    "})\n",
    "\n",
    "submission_filename = 'submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfa2524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:49:39.938058Z",
     "iopub.status.busy": "2025-10-24T19:49:39.937702Z",
     "iopub.status.idle": "2025-10-24T19:49:39.952468Z",
     "shell.execute_reply": "2025-10-24T19:49:39.951615Z"
    },
    "papermill": {
     "duration": 0.020824,
     "end_time": "2025-10-24T19:49:39.954376",
     "exception": false,
     "start_time": "2025-10-24T19:49:39.933552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Hospital_Id  Transport_Cost\n",
      "0          fffe33003400      220.234877\n",
      "1  fffe3700330036003600      239.301549\n",
      "2  fffe3300390038003400     2638.565183\n",
      "3      fffe310030003900      121.117436\n",
      "4  fffe3700330031003200      574.101602\n"
     ]
    }
   ],
   "source": [
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14140597,
     "sourceId": 116098,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.138554,
   "end_time": "2025-10-24T19:49:42.576783",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-24T19:49:25.438229",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
